{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b243f018-2c18-476a-82a6-10cdbce9e963",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here, we extract the test loss from the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b91824-82ff-4a5a-9a9c-434fa49310c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Local package imports\n",
    "from minima_volume.dataset_funcs import (\n",
    "    load_dataset,\n",
    "    load_model,\n",
    "    load_models_and_data,\n",
    "    prepare_datasets,\n",
    "    tensor_to_list,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6af0fc-d0a3-4793-8ca0-33c9461c6777",
   "metadata": {},
   "source": [
    "## Model + Dataset Specific Code\n",
    "\n",
    "This is for model and dataset specific code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389d62b5-59f9-43ff-9c59-8e57e3d39f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specifies the model module name\n",
    "from minima_volume.models import modulo_arithmetic_model_data as model_module\n",
    "modulus = 97\n",
    "\n",
    "# MNIST specific initialization parameters\n",
    "hidden_dims = [250]\n",
    "\n",
    "# Grab model\n",
    "model_template = model_module.get_model(N = modulus, hidden_dims=hidden_dims, device=device, seed=0)\n",
    "\n",
    "# Grab loss and metrics\n",
    "loss_fn = model_module.get_loss_fn()\n",
    "other_metrics = model_module.get_additional_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444f632-4ed7-4f3d-9f18-19a6cedabed8",
   "metadata": {},
   "source": [
    "## Loading Model and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47dff34d-5a53-49d0-a3df-d1306cecd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for models and dataset in: models_and_data\n",
      "Found 9 model files:\n",
      "  - model_epoch_1000.pt\n",
      "  - model_epoch_1500.pt\n",
      "  - model_epoch_200.pt\n",
      "  - model_epoch_2000.pt\n",
      "  - model_epoch_3000.pt\n",
      "  - model_epoch_400.pt\n",
      "  - model_epoch_4000.pt\n",
      "  - model_epoch_600.pt\n",
      "  - model_epoch_800.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_1000.pt\n",
      "Successfully loaded: model_epoch_1000.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_1500.pt\n",
      "Successfully loaded: model_epoch_1500.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_200.pt\n",
      "Successfully loaded: model_epoch_200.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_2000.pt\n",
      "Successfully loaded: model_epoch_2000.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_3000.pt\n",
      "Successfully loaded: model_epoch_3000.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_400.pt\n",
      "Successfully loaded: model_epoch_400.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_4000.pt\n",
      "Successfully loaded: model_epoch_4000.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_600.pt\n",
      "Successfully loaded: model_epoch_600.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_800.pt\n",
      "Successfully loaded: model_epoch_800.pt\n",
      "\n",
      "Model data loaded from all models:\n",
      "Model 0 (model_epoch_1000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 1000 entries\n",
      "  - Test accuracies: 1000 entries\n",
      "Model 1 (model_epoch_1500.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 1500 entries\n",
      "  - Test accuracies: 1500 entries\n",
      "Model 2 (model_epoch_200.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 3 (model_epoch_2000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 2000 entries\n",
      "  - Test accuracies: 2000 entries\n",
      "Model 4 (model_epoch_3000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 3000 entries\n",
      "  - Test accuracies: 3000 entries\n",
      "Model 5 (model_epoch_400.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 400 entries\n",
      "  - Test accuracies: 400 entries\n",
      "Model 6 (model_epoch_4000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 4000 entries\n",
      "  - Test accuracies: 4000 entries\n",
      "Model 7 (model_epoch_600.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 600 entries\n",
      "  - Test accuracies: 600 entries\n",
      "Model 8 (model_epoch_800.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 800 entries\n",
      "  - Test accuracies: 800 entries\n",
      "\n",
      "Loading dataset...\n",
      "Using dataset file: dataset.pt\n",
      "✅ Dataset loaded from models_and_data\\dataset.pt\n",
      "Dataset type: data\n",
      "Dataset quantities: [0]\n",
      "\n",
      "Tensor shapes:\n",
      "  x_base_train: torch.Size([3104, 194])\n",
      "  y_base_train: torch.Size([3104, 97])\n",
      "  x_additional: torch.Size([0, 194])\n",
      "  y_additional: torch.Size([0, 97])\n",
      "  x_test: torch.Size([9409, 194])\n",
      "  y_test: torch.Size([9409, 97])\n",
      "Reconstructed 9 trained models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_4000.pt\n",
      "Successfully loaded: model_epoch_4000.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_600.pt\n",
      "Successfully loaded: model_epoch_600.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_epoch_800.pt\n",
      "Successfully loaded: model_epoch_800.pt\n",
      "\n",
      "Model data loaded from all models:\n",
      "Model 0 (model_epoch_1000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 1000 entries\n",
      "  - Test accuracies: 1000 entries\n",
      "Model 1 (model_epoch_1500.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 1500 entries\n",
      "  - Test accuracies: 1500 entries\n",
      "Model 2 (model_epoch_200.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 3 (model_epoch_2000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 2000 entries\n",
      "  - Test accuracies: 2000 entries\n",
      "Model 4 (model_epoch_3000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 3000 entries\n",
      "  - Test accuracies: 3000 entries\n",
      "Model 5 (model_epoch_400.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 400 entries\n",
      "  - Test accuracies: 400 entries\n",
      "Model 6 (model_epoch_4000.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 4000 entries\n",
      "  - Test accuracies: 4000 entries\n",
      "Model 7 (model_epoch_600.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 600 entries\n",
      "  - Test accuracies: 600 entries\n",
      "Model 8 (model_epoch_800.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 800 entries\n",
      "  - Test accuracies: 800 entries\n",
      "\n",
      "Loading dataset...\n",
      "Using dataset file: dataset.pt\n",
      "✅ Dataset loaded from models_and_data\\dataset.pt\n",
      "Dataset type: data\n",
      "Dataset quantities: [0]\n",
      "\n",
      "Tensor shapes:\n",
      "  x_base_train: torch.Size([3104, 194])\n",
      "  y_base_train: torch.Size([3104, 97])\n",
      "  x_additional: torch.Size([0, 194])\n",
      "  y_additional: torch.Size([0, 97])\n",
      "  x_test: torch.Size([9409, 194])\n",
      "  y_test: torch.Size([9409, 97])\n",
      "Reconstructed 9 trained models\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Load Trained Models and Dataset\n",
    "# ====================================\n",
    "target_dir = \"models_and_data\"  # relative path\n",
    "loaded_models, loaded_model_data, loaded_dataset = load_models_and_data(\n",
    "    model_template=model_template,\n",
    "    target_dir=target_dir,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Dataset Info\n",
    "dataset_type = loaded_dataset['dataset_type']\n",
    "print(f\"Dataset type: {dataset_type}\")\n",
    "print(f\"Dataset quantities: {loaded_dataset['dataset_quantities']}\")\n",
    "\n",
    "print(\"\\nTensor shapes:\")\n",
    "for key in [\"x_base_train\", \"y_base_train\", \"x_additional\", \"y_additional\", \"x_test\", \"y_test\"]:\n",
    "    shape = getattr(loaded_dataset[key], \"shape\", None)\n",
    "    print(f\"  {key}: {shape if shape is not None else 'None'}\")\n",
    "\n",
    "# Reconstruct trained_model dicts safely.\n",
    "# If the loss or accuracy or additional metrics happen to be\n",
    "# tensors, they get safely converted to lists.\n",
    "all_models = [\n",
    "    {\n",
    "        \"model\": model,\n",
    "        **{\n",
    "            k: tensor_to_list(model_data[k], key_path=k)\n",
    "            for k in [\"train_loss\", \"train_accs\", \"test_loss\", \"test_accs\", \"additional_data\", \"dataset_type\", \"total_epochs_trained\"]\n",
    "        },\n",
    "    }\n",
    "    for model, model_data in zip(loaded_models, loaded_model_data)\n",
    "]\n",
    "print(f\"Reconstructed {len(all_models)} trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9831c1e4-844b-42c2-a951-b60aab26cb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print (all_models[0]['total_epochs_trained'])\n",
    "print (all_models[1]['total_epochs_trained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530ae37-14b3-4d87-a88f-b8c868788a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
