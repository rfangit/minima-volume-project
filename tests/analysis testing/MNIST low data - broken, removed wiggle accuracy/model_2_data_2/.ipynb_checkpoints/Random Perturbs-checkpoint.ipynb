{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4494701a-38ed-4662-8e6b-66b8e14fcadf",
   "metadata": {},
   "source": [
    "# Random Perturbations\n",
    "\n",
    "This notebook is a streamlined notebook for evaluating random perturbations on saved models and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243f018-2c18-476a-82a6-10cdbce9e963",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b91824-82ff-4a5a-9a9c-434fa49310c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "# Importing our existing funcs\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "# Import modules\n",
    "\n",
    "from perturb_funcs import ( analyze_wiggles_metrics )\n",
    "\n",
    "from dataset_funcs import ( prepare_datasets,\n",
    "                            save_model, save_dataset,\n",
    "                            load_model, load_dataset,\n",
    "                            load_models_and_data)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d7d91-a84f-4acd-9ea4-ce6c8b45ed39",
   "metadata": {},
   "source": [
    "## Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb0358d-193d-4228-8a73-baad9a52d6e0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Perturbation Configuration\n",
    "perturbation_seed = 1\n",
    "num_directions = 2000\n",
    "N = 100\n",
    "x = np.linspace(0, 1, N)\n",
    "coefficients = x**2\n",
    "\n",
    "# Perturbation Dataset Configuration\n",
    "# We use all of the base data size\n",
    "# but only some of the additional data\n",
    "dataset_quantities = [0, 600 - 60, 6000 - 60, 60000 - 60]\n",
    "\n",
    "# Base output directory\n",
    "base_output_dir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6af0fc-d0a3-4793-8ca0-33c9461c6777",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389d62b5-59f9-43ff-9c59-8e57e3d39f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Model Definition\n",
    "class NetMLP(nn.Module):\n",
    "    def __init__(self, seed=None):\n",
    "        super(NetMLP, self).__init__()\n",
    "        if seed is not None:\n",
    "            self.seed = seed\n",
    "            with torch.random.fork_rng():\n",
    "                torch.manual_seed(seed)\n",
    "                self._initialize_layers()\n",
    "        else:\n",
    "            self._initialize_layers()\n",
    "\n",
    "    def _initialize_layers(self):\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)        # Flatten input (B, 1, 28, 28) → (B, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No softmax here\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444f632-4ed7-4f3d-9f18-19a6cedabed8",
   "metadata": {},
   "source": [
    "## Loading Model and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47dff34d-5a53-49d0-a3df-d1306cecd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for models and dataset in: L:\\Programming\\ARC\\minima_sizes\\MNIST\\experiments - low data\\model_hidden_2000\\Example Folder\\models_and_data\n",
      "Found 4 model files:\n",
      "  - model_additional_0.pt\n",
      "  - model_additional_540.pt\n",
      "  - model_additional_5940.pt\n",
      "  - model_additional_59940.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_0.pt\n",
      "Successfully loaded: model_additional_0.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_540.pt\n",
      "Successfully loaded: model_additional_540.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_5940.pt\n",
      "Successfully loaded: model_additional_5940.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_59940.pt\n",
      "Successfully loaded: model_additional_59940.pt\n",
      "\n",
      "Model data loaded from all models:\n",
      "Model 0 (model_additional_0.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 500 entries\n",
      "  - Test accuracies: 500 entries\n",
      "Model 1 (model_additional_540.pt):\n",
      "  - Additional data: 540\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 500 entries\n",
      "  - Test accuracies: 500 entries\n",
      "Model 2 (model_additional_5940.pt):\n",
      "  - Additional data: 5940\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 500 entries\n",
      "  - Test accuracies: 500 entries\n",
      "Model 3 (model_additional_59940.pt):\n",
      "  - Additional data: 59940\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 500 entries\n",
      "  - Test accuracies: 500 entries\n",
      "\n",
      "Loading dataset...\n",
      "✅ Dataset loaded from models_and_data\\dataset.pt\n",
      "Dataset type: data\n",
      "Dataset quantities: [0, 540, 5940, 59940]\n",
      "\n",
      "Tensor shapes:\n",
      "  x_base_train: torch.Size([60, 1, 28, 28])\n",
      "  y_base_train: torch.Size([60])\n",
      "  x_additional: torch.Size([59940, 1, 28, 28])\n",
      "  y_additional: torch.Size([59940])\n",
      "  x_test: torch.Size([10000, 1, 28, 28])\n",
      "  y_test: torch.Size([10000])\n",
      "Reconstructed 4 trained models\n"
     ]
    }
   ],
   "source": [
    "# Get the relative path\n",
    "target_dir = Path(\"models_and_data\") #current directory\n",
    "\n",
    "# Lists to store loaded models and additional data\n",
    "model_template = NetMLP().to(device)\n",
    "loaded_models, loaded_model_data, loaded_dataset = load_models_and_data(model_template=model_template, target_dir=target_dir, device=device)\n",
    "# Print dataset information\n",
    "dataset_type = loaded_dataset['dataset_type']\n",
    "print(f\"Dataset type: {dataset_type}\")\n",
    "print(f\"Dataset quantities: {loaded_dataset['dataset_quantities']}\")\n",
    "\n",
    "# Print tensor shapes\n",
    "print(\"\\nTensor shapes:\")\n",
    "for key in [\"x_base_train\", \"y_base_train\", \"x_additional\", \"y_additional\", \"x_test\", \"y_test\"]:\n",
    "    if loaded_dataset[key] is not None:\n",
    "        print(f\"  {key}: {loaded_dataset[key].shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: None\")\n",
    "\n",
    "# Reconstruct the trained_model structure for each model, to pass into perturbations\n",
    "all_models = []\n",
    "for i, (model, model_data) in enumerate(zip(loaded_models, loaded_model_data)):\n",
    "    trained_model = {\n",
    "        'model': model,\n",
    "        'train_losses': model_data['train_losses'],\n",
    "        'train_accs': model_data['train_accs'],\n",
    "        'test_losses': model_data['test_losses'],\n",
    "        'test_accs': model_data['test_accs'],\n",
    "        'additional_data': model_data['additional_data'],\n",
    "        'dataset_type': model_data['dataset_type'],\n",
    "    }\n",
    "    all_models.append(trained_model)\n",
    "\n",
    "# Now all_models has the same structure as your original trained_model list\n",
    "print(f\"Reconstructed {len(all_models)} trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbbaed-0824-4e7f-adb2-5bb7acb686d8",
   "metadata": {},
   "source": [
    "### Prepare For Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b75e36-9d45-475c-9825-d9ad852e988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train = loaded_dataset['x_base_train']\n",
    "y_base_train = loaded_dataset['y_base_train']\n",
    "x_additional = loaded_dataset['x_additional']\n",
    "y_additional = loaded_dataset['y_additional']\n",
    "\n",
    "x_test = loaded_dataset['x_test']\n",
    "y_test = loaded_dataset['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e6661-c42d-43b7-9ddd-291d56ff910f",
   "metadata": {},
   "source": [
    "## Perturbations\n",
    "\n",
    "We perform perturbations, sending in the models trained along with parameters to reproduce a small number of the training datasets, and evaluating their training and test losses over perturbations.\n",
    "\n",
    "For a large dataset, we'll use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1b83dd-2850-45f2-b9b3-e7ead77b93b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters of the perturbation is 235146\n",
      "Testing on data with 0 samples - 20 directions\n",
      "Testing model trained on 0 additional data.\n",
      "Test performance - Loss: 1.9667, Accuracy: 0.6860\n",
      "Wiggle completed in 1.93 seconds for data model trained with 0 samples\n",
      "Saved to data_0\n",
      "\n",
      "Testing model trained on 540 additional data.\n",
      "Test performance - Loss: 0.7910, Accuracy: 0.8752\n",
      "Wiggle completed in 1.76 seconds for data model trained with 540 samples\n",
      "Saved to data_0\n",
      "\n",
      "Testing model trained on 5940 additional data.\n",
      "Test performance - Loss: 0.3433, Accuracy: 0.9426\n",
      "Wiggle completed in 1.71 seconds for data model trained with 5940 samples\n",
      "Saved to data_0\n",
      "\n",
      "Testing model trained on 59940 additional data.\n",
      "Test performance - Loss: 0.0985, Accuracy: 0.9762\n",
      "Wiggle completed in 1.71 seconds for data model trained with 59940 samples\n",
      "Saved to data_0\n",
      "\n",
      "Testing on data with 540 samples - 20 directions\n",
      "Testing model trained on 540 additional data.\n",
      "Test performance - Loss: 0.7910, Accuracy: 0.8752\n",
      "Wiggle completed in 1.71 seconds for data model trained with 540 samples\n",
      "Saved to data_540\n",
      "\n",
      "Testing model trained on 5940 additional data.\n",
      "Test performance - Loss: 0.3433, Accuracy: 0.9426\n",
      "Wiggle completed in 1.71 seconds for data model trained with 5940 samples\n",
      "Saved to data_540\n",
      "\n",
      "Testing model trained on 59940 additional data.\n",
      "Test performance - Loss: 0.0985, Accuracy: 0.9762\n",
      "Wiggle completed in 1.70 seconds for data model trained with 59940 samples\n",
      "Saved to data_540\n",
      "\n",
      "Testing on data with 5940 samples - 20 directions\n",
      "Testing model trained on 5940 additional data.\n",
      "Test performance - Loss: 0.3433, Accuracy: 0.9426\n",
      "Wiggle completed in 1.91 seconds for data model trained with 5940 samples\n",
      "Saved to data_5940\n",
      "\n",
      "Testing model trained on 59940 additional data.\n",
      "Test performance - Loss: 0.0985, Accuracy: 0.9762\n",
      "Wiggle completed in 1.75 seconds for data model trained with 59940 samples\n",
      "Saved to data_5940\n",
      "\n",
      "Testing on data with 59940 samples - 20 directions\n",
      "Testing model trained on 59940 additional data.\n",
      "Test performance - Loss: 0.0985, Accuracy: 0.9762\n",
      "Wiggle completed in 5.69 seconds for data model trained with 59940 samples\n",
      "Saved to data_59940\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Our saved results are structured as follows:\\nwiggle_results: List of dictionaries containing wiggle test results\\nEach dictionary is of the form\\n{\\n\\'losses\\':\\n\\'coefficients\\':\\n\\'accuracies\\':\\n\\'perturbation_seed\\':\\n\\'perturbation_norm\\':\\n}\\nmodel: PyTorch model used in analysis (state_dict will be saved)\\noutput_dir: Directory to save results (default: \"imgs/swiss/random_dirs\")\\nfilename: Name of output file (default: \"random_directions.npz\")\\n**kwargs: Additional key-value pairs to be saved in the output file\\nTypically:\\n\\'additional_data\\':\\n\\'model_trained_data\\':\\n\\'dataset_type\\':\\n\\'base_dataset_size\\': \\n\\'test_loss\\':\\n\\'test_accuracy\\':\\n\\'num_params\\':\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    return criterion(logits, labels)\n",
    "\n",
    "def accuracy_fn(logits, labels):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == labels).float().mean()\n",
    "\n",
    "analyze_wiggles_metrics(model_list = all_models, \n",
    "                x_base_train = x_base_train, y_base_train = y_base_train, \n",
    "                x_additional = x_additional, y_additional = y_additional,\n",
    "                x_test = x_test, y_test = y_test, \n",
    "                dataset_quantities = dataset_quantities, \n",
    "                dataset_type = dataset_type, \n",
    "                metrics={\"losses\": loss_fn, \"accuracies\": accuracy_fn},\n",
    "                coefficients = coefficients,\n",
    "                num_directions = num_directions,\n",
    "                perturbation_seed = perturbation_seed,\n",
    "                base_output_dir = base_output_dir, #f\"results/test_swiss_model_{model_seed}_data_{data_seed}\"\n",
    "                device = device) #instead of device\n",
    "\n",
    "\"\"\" Our saved results are structured as follows:\n",
    "wiggle_results: List of dictionaries containing wiggle test results\n",
    "Each dictionary is of the form\n",
    "{\n",
    "'losses':\n",
    "'coefficients':\n",
    "'accuracies':\n",
    "'perturbation_seed':\n",
    "'perturbation_norm':\n",
    "}\n",
    "model: PyTorch model used in analysis (state_dict will be saved)\n",
    "output_dir: Directory to save results (default: \"imgs/swiss/random_dirs\")\n",
    "filename: Name of output file (default: \"random_directions.npz\")\n",
    "**kwargs: Additional key-value pairs to be saved in the output file\n",
    "Typically:\n",
    "'additional_data':\n",
    "'model_trained_data':\n",
    "'dataset_type':\n",
    "'base_dataset_size': \n",
    "'test_loss':\n",
    "'test_accuracy':\n",
    "'num_params':\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530ae37-14b3-4d87-a88f-b8c868788a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
