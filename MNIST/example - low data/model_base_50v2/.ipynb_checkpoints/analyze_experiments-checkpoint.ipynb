{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1bf684-d8fa-4de8-b136-a894b6f0f176",
   "metadata": {},
   "source": [
    "# Comparisons In A Given Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9c9cf7-eb85-456f-b84e-e94d457fe560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry: Model_0, base_train_size = 60\n",
      "Experiment folders:  ['model_0_data_10', 'model_1_data_11', 'model_2_data_12', 'model_3_data_13', 'model_4_data_14', 'model_5_data_15', 'model_6_data_16', 'model_7_data_17', 'model_8_data_18', 'model_9_data_19']\n",
      "Different data levels where loss landscapes were computed: ['data_0', 'data_1940', 'data_19940', 'data_540', 'data_5940', 'data_59940']\n",
      "The base train size is:  60\n",
      "Data levels where models were trained:  [0, 1940, 19940, 540, 5940, 59940]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from minima_volume.analysis_funcs import (  grab_folder_names,\n",
    "                                            load_base_train_size,\n",
    "                                            list_additional_data,\n",
    "                                            multiple_minima_fixed_landscape,\n",
    "                                            fixed_landscape_minima_labels,\n",
    "                                            plot_fixed_landscape_minima_pair,\n",
    "                                            plot_perturb_probs,\n",
    "                                            model_volume_across_targets,\n",
    "                                            append_cutoff_points,\n",
    "                                            varying_landscape_minima_labels,\n",
    "                                            plot_minima_volume_vs_data_level,\n",
    "                                            multiple_minima_fixed_landscape_perturb_probs,\n",
    "                    \n",
    ")\n",
    "\n",
    "loss_value = 0.1  # Loss value to analyze\n",
    "data_level_log = False #Is the data log distributed?\n",
    "base_output_dir = \"analysis\"  # Root folder to save all plots\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Data for the loss landscape levels\n",
    "experiment_folders, data_modifications = grab_folder_names()\n",
    "\n",
    "base_train_size = load_base_train_size(experiment_folders[0])\n",
    "model_data_sizes = list_additional_data(experiment_folders[0])\n",
    "\n",
    "base_shift = 0\n",
    "if data_modifications[0].startswith(\"data_\"):\n",
    "    base_shift = base_train_size  # Shift to reflect true dataset size\n",
    "\n",
    "print(\"Experiment folders: \", experiment_folders)\n",
    "print(\"Different data levels where loss landscapes were computed:\", data_modifications)\n",
    "print(\"The base train size is: \", base_train_size)\n",
    "print(\"Data levels where models were trained: \", model_data_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7f87f-9623-4855-84db-09f38d813958",
   "metadata": {},
   "source": [
    "## Plotting Different Model Volumes In A Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248f4371-dbed-4dfe-83ae-96ed3816a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data modification folder: data_0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'subset_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m all_mdl, all_log_rn, all_test_loss \u001b[38;5;241m=\u001b[39m multiple_minima_fixed_landscape(\n\u001b[0;32m     13\u001b[0m     experiment_folders, data_modification, loss_value \u001b[38;5;66;03m#mdl is modification data level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m number_of_minima \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_mdl[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# Not used currently\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m all_subset_sizes, all_perturb_probs, all_perturb_probs_full \u001b[38;5;241m=\u001b[39m \u001b[43mmultiple_minima_fixed_landscape_perturb_probs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_folders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_modification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#mdl is modification data level\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m all_mdl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_mdl)\n\u001b[0;32m     22\u001b[0m all_log_rn \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_log_rn)\n",
      "File \u001b[1;32mL:\\Programming\\ARC\\minima_volume_project\\minima_volume\\analysis_funcs.py:258\u001b[0m, in \u001b[0;36mmultiple_minima_fixed_landscape_perturb_probs\u001b[1;34m(experiment_folders, target_data_modification, loss_value, selected_folders)\u001b[0m\n\u001b[0;32m    255\u001b[0m all_subset_sizes, all_perturb_probs, all_perturb_probs_full \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_folder \u001b[38;5;129;01min\u001b[39;00m folders_to_use:\n\u001b[1;32m--> 258\u001b[0m     subset_sizes, perturb_probs, perturb_probs_full \u001b[38;5;241m=\u001b[39m \u001b[43mload_perturb_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data_modification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     all_subset_sizes\u001b[38;5;241m.\u001b[39mappend(subset_sizes)\n\u001b[0;32m    260\u001b[0m     all_perturb_probs\u001b[38;5;241m.\u001b[39mappend(perturb_probs)\n",
      "File \u001b[1;32mL:\\Programming\\ARC\\minima_volume_project\\minima_volume\\analysis_funcs.py:138\u001b[0m, in \u001b[0;36mload_perturb_probs\u001b[1;34m(model_folder, data_modification, loss_value)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    136\u001b[0m     results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m--> 138\u001b[0m subset_sizes_values \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubset_sizes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    139\u001b[0m radii_rank_probabilities_values \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mradii_rank_probabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    140\u001b[0m radii_rank_probabilities_full_levels \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mradii_rank_probabilities_full\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'subset_sizes'"
     ]
    }
   ],
   "source": [
    "for data_modification in data_modifications:\n",
    "    print(f\"\\nProcessing data modification folder: {data_modification}\")\n",
    "\n",
    "    # Create output folder for this data_modification\n",
    "    save_dir = os.path.join(base_output_dir, data_modification)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Get axis labels and titles\n",
    "    labels = fixed_landscape_minima_labels(data_modification, base_train_size)\n",
    "\n",
    "    # Extract minima landscapes\n",
    "    all_mdl, all_log_rn, all_test_loss = multiple_minima_fixed_landscape(\n",
    "        experiment_folders, data_modification, loss_value #mdl is modification data level\n",
    "    )\n",
    "    number_of_minima = len(all_mdl[0]) # Not used currently\n",
    "\n",
    "    all_mdl = np.array(all_mdl)\n",
    "    all_log_rn = np.array(all_log_rn)\n",
    "    all_test_loss = np.array(all_test_loss)\n",
    "    \n",
    "    # Print types and shapes\n",
    "    for name, var in [(\"all_mdl\", all_mdl), (\"all_log_rn\", all_log_rn), (\"all_test_loss\", all_test_loss)]:\n",
    "        print(f\"{name}: type={type(var)}, shape={var.shape}\")\n",
    "    \n",
    "    # Central tendency runs: mean and median\n",
    "    for ct, suffix in [(\"mean\", \"_avg\"), (\"median\", \"_median\")]:\n",
    "        for ranking in [False, True]:\n",
    "            out_dir = save_dir if not ranking else os.path.join(save_dir, \"ranks\") #handles ranking\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            # --- Data-level plots: loop through shaded + errorbar ---\n",
    "            #for avg_style in [\"shaded\", \"errorbar\"]:\n",
    "            #    style_suffix = \"_shaded\" if avg_style == \"shaded\" else \"_errbar\"\n",
    "            style_suffix = \"_errbar\"\n",
    "            avg_style = \"errorbar\"\n",
    "\n",
    "            # --- Data level vs Log Volume ---\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_mdl, all_log_rn,\n",
    "                xlabel=labels[\"xlabel\"], ylabel=\"Log Volume\",\n",
    "                title=labels[\"title_volume\"],\n",
    "                log_scale=data_level_log,\n",
    "                ranking=ranking,\n",
    "                alpha=0.5,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"data_level_vs_log_volume\" + suffix + style_suffix,\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=avg_style,\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=(avg_style == \"errorbar\"),\n",
    "                xlabel_size=14, ylabel_size=14, title_size=16,\n",
    "                base_shift=base_shift,\n",
    "            )\n",
    "\n",
    "            # --- Data level vs Test Loss ---\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_mdl, all_test_loss,\n",
    "                xlabel=labels[\"xlabel\"], ylabel=\"Test Loss\",\n",
    "                title=labels[\"title_loss\"],\n",
    "                log_scale=False,\n",
    "                ranking=ranking,\n",
    "                alpha=0.5,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"data_level_vs_test_loss\" + suffix + style_suffix,\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=avg_style,\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=(avg_style == \"errorbar\"),\n",
    "                xlabel_size=14, ylabel_size=14, title_size=16,\n",
    "                base_shift=base_shift,\n",
    "            )\n",
    "\n",
    "            # --- Log Volume vs Test Loss ---\n",
    "            # Only error bar version (no shaded version)\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_log_rn, all_test_loss,\n",
    "                xlabel=\"Log Volume\", ylabel=\"Test Loss\",\n",
    "                title=labels[\"title_loss\"],\n",
    "                log_scale=False,\n",
    "                ranking=ranking,\n",
    "                alpha=0.5,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"log_volume_vs_test_loss\" + suffix + \"_errbar\",\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=\"errorbar\",\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=True,  # allow x-error bars\n",
    "                xlabel_size=14, ylabel_size=14, title_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63796f34-a5f0-4c4e-9088-6169971bd407",
   "metadata": {},
   "source": [
    "# Aggregrate Test Accuracy\n",
    "\n",
    "Test accuracy vs dataset size across all plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2503cd-f923-4797-9b85-75962b48b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_folders = [folder for folder in experiment_folders\n",
    "                   if not os.path.exists(os.path.join(folder, \"test_accuracies.npz\"))]\n",
    "\n",
    "if missing_folders:\n",
    "    print(f\"⚠️ Skipping aggregation — missing test_accuracies.npz in: {missing_folders}\")\n",
    "else:\n",
    "    # ---------- Step 2: Load all data ----------\n",
    "    all_dataset_sizes = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    for folder in experiment_folders:\n",
    "        file_path = os.path.join(folder, \"test_accuracies.npz\")\n",
    "        data = np.load(file_path)\n",
    "        all_dataset_sizes.extend(data[\"dataset_sizes\"])\n",
    "        all_accuracies.extend(data[\"final_test_accuracies\"])\n",
    "\n",
    "    all_dataset_sizes = np.array(all_dataset_sizes)\n",
    "    all_accuracies = np.array(all_accuracies)\n",
    "\n",
    "    # ---------- Step 3: Group by dataset size ----------\n",
    "    grouped_data = defaultdict(list)\n",
    "    for size, acc in zip(all_dataset_sizes, all_accuracies):\n",
    "        grouped_data[size].append(acc)\n",
    "\n",
    "    sorted_sizes = np.array(sorted(grouped_data.keys()))\n",
    "    mean_accs = np.array([np.mean(grouped_data[size]) for size in sorted_sizes])\n",
    "    std_accs = np.array([np.std(grouped_data[size]) for size in sorted_sizes])\n",
    "\n",
    "    # ---------- Step 4: Plot with error bars ----------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(\n",
    "        sorted_sizes, mean_accs, yerr=std_accs,\n",
    "        fmt='o-', capsize=4, linewidth=2, markersize=8,\n",
    "        color='tab:blue', ecolor='black', elinewidth=1, alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Total Dataset Size\", fontsize=12)\n",
    "    plt.ylabel(\"Final Test Accuracy\", fontsize=12)\n",
    "    plt.title(\"Test Accuracy vs. Dataset Size (Mean ± Std)\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim((0.5, 1.0))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(base_output_dir, \"test_acc_plot.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✅ Plot saved to {plot_path}\")\n",
    "\n",
    "    # ---------- Step 5: Save combined data ----------\n",
    "    save_path = os.path.join(base_output_dir, \"test_acc.npz\")\n",
    "    np.savez(\n",
    "        save_path,\n",
    "        dataset_sizes=sorted_sizes,\n",
    "        mean_accuracies=mean_accs,\n",
    "        std_accuracies=std_accs,\n",
    "        all_dataset_sizes=all_dataset_sizes,\n",
    "        all_accuracies=all_accuracies\n",
    "    )\n",
    "    print(f\"✅ Aggregated data saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f267ad-e4e1-4750-9944-25f8a9087a07",
   "metadata": {},
   "source": [
    "# Comparisons Across Loss Landscapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba89c53-f00e-4cdb-86c5-aaaf87e2cf16",
   "metadata": {},
   "source": [
    "Here, we track the performance of all models across all loss landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5ebe9-679a-43e0-9056-58de5b3779b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the poisoning experiments, this plot has no meaning\n",
    "\n",
    "labels = varying_landscape_minima_labels(\n",
    "    data_modification_folder=data_modifications[0], # Random choice to grab datatype from\n",
    "    base_train_size=base_train_size,\n",
    ")\n",
    "\n",
    "results_dict = model_volume_across_targets(\n",
    "    target_model_data_levels=model_data_sizes,\n",
    "    loss_value=loss_value,\n",
    "    experiment_folders=experiment_folders\n",
    ")\n",
    "results_with_cutoff = append_cutoff_points(results_dict, threshold=loss_value, base_dir=\".\")\n",
    "\n",
    "plot_minima_volume_vs_data_level(\n",
    "    results_dict = results_with_cutoff,\n",
    "    data_type=labels[\"data_type\"],          # NEW: needed for correct legend labels\n",
    "    base_train_size=base_train_size,  # NEW: used for dataset size shifts\n",
    "    xlabel=labels[\"xlabel\"],\n",
    "    ylabel=\"Log Volume\",\n",
    "    title=labels[\"title_volume\"],\n",
    "    log_scale = data_level_log,\n",
    "    alpha=0.1,\n",
    "    plot_average=True,\n",
    "    output_dir=base_output_dir, filename=\"log_volumes_vs_data_levels\",\n",
    "    xlabel_size=15, ylabel_size=15, title_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a7111-d900-4110-8d65-0558d8e4c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_minima_volume_vs_data_level(\n",
    "    results_dict = results_with_cutoff,\n",
    "    data_type=labels[\"data_type\"],          # NEW: needed for correct legend labels\n",
    "    base_train_size=base_train_size,  # NEW: used for dataset size shifts\n",
    "    xlabel=labels[\"xlabel\"],\n",
    "    ylabel=\"Log Volume\",\n",
    "    title=labels[\"title_volume\"],\n",
    "    log_scale = True,\n",
    "    alpha=0.1,\n",
    "    plot_average=True,\n",
    "    output_dir=base_output_dir, filename=\"log_volumes_vs_data_levels_log\",\n",
    "    xlabel_size=15, ylabel_size=15, title_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cc389-e620-4444-b261-88b0adc41831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
