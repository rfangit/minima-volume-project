{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f83e899-d087-413c-a9a1-66b46e7a10f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 folders for Volume/Test Accuracy notebooks.\n",
      "Will run these notebooks: ['Volume Estimation Pipeline.ipynb']\n",
      "üîπ Running model_0_data_10\\Volume Estimation Pipeline.ipynb...\n",
      "üîπ Running model_1_data_11\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_0_data_10/Volume Estimation Pipeline.ipynb (took 116.80s)\n",
      "‚úÖ SUCCESS: model_1_data_11/Volume Estimation Pipeline.ipynb (took 117.27s)\n",
      "üîπ Running model_2_data_12\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_2_data_12/Volume Estimation Pipeline.ipynb (took 113.61s)\n",
      "üîπ Running model_3_data_13\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_3_data_13/Volume Estimation Pipeline.ipynb (took 112.86s)\n",
      "üîπ Running model_4_data_14\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_4_data_14/Volume Estimation Pipeline.ipynb (took 132.57s)\n",
      "üîπ Running model_5_data_15\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_5_data_15/Volume Estimation Pipeline.ipynb (took 114.96s)\n",
      "üîπ Running model_6_data_16\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_6_data_16/Volume Estimation Pipeline.ipynb (took 101.01s)\n",
      "üîπ Running model_7_data_17\\Volume Estimation Pipeline.ipynb...\n",
      "üîπ Running model_8_data_18\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_7_data_17/Volume Estimation Pipeline.ipynb (took 91.22s)\n",
      "‚úÖ SUCCESS: model_8_data_18/Volume Estimation Pipeline.ipynb (took 91.23s)\n",
      "üîπ Running model_9_data_19\\Volume Estimation Pipeline.ipynb...\n",
      "‚úÖ SUCCESS: model_9_data_19/Volume Estimation Pipeline.ipynb (took 90.79s)\n",
      "‚úÖ All done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_0_data_10/Test Accuracy.ipynb (took 7.85s)\n",
      "üîπ Running model_0_data_10\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_0_data_10/Volume Cutoff.ipynb (took 9.55s)\n",
      "üîπ Running model_0_data_10\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_0_data_10/Volume Estimation Pipeline.ipynb (took 64.05s)\n",
      "üîπ Running model_1_data_11\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Running model_1_data_11\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_1_data_11/Test Accuracy.ipynb (took 7.50s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Running model_1_data_11\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_1_data_11/Volume Cutoff.ipynb (took 9.67s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_1_data_11/Volume Estimation Pipeline.ipynb (took 64.88s)\n",
      "üîπ Running model_2_data_12\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_2_data_12/Test Accuracy.ipynb (took 7.49s)\n",
      "üîπ Running model_2_data_12\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_2_data_12/Volume Cutoff.ipynb (took 9.46s)\n",
      "üîπ Running model_2_data_12\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_2_data_12/Volume Estimation Pipeline.ipynb (took 64.70s)\n",
      "üîπ Running model_3_data_13\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_3_data_13/Test Accuracy.ipynb (took 7.51s)\n",
      "üîπ Running model_3_data_13\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_3_data_13/Volume Cutoff.ipynb (took 9.34s)\n",
      "üîπ Running model_3_data_13\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_3_data_13/Volume Estimation Pipeline.ipynb (took 64.15s)\n",
      "üîπ Running model_4_data_14\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_4_data_14/Test Accuracy.ipynb (took 7.56s)\n",
      "üîπ Running model_4_data_14\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_4_data_14/Volume Cutoff.ipynb (took 9.54s)\n",
      "üîπ Running model_4_data_14\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_4_data_14/Volume Estimation Pipeline.ipynb (took 65.19s)\n",
      "üîπ Running model_5_data_15\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_5_data_15/Test Accuracy.ipynb (took 7.47s)\n",
      "üîπ Running model_5_data_15\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: model_5_data_15/Volume Cutoff.ipynb - [NbConvertApp] Converting notebook model_5_data_15\\Volume Cutoff.ipynb to notebook\n",
      "L:\\Programming\\diffusion-env\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"L:\\Programming\\diffusion-env\\Scripts\\jupyter-nbconvert.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 1062, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 918, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "# -------------------------------\n",
      "# 1. Build the full dataset once\n",
      "# -------------------------------\n",
      "# Explicitly extract tensors from the loaded dataset\n",
      "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
      "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
      "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
      "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
      "x_test = loaded_dataset.get(\"x_test\", None)\n",
      "y_test = loaded_dataset.get(\"y_test\", None)\n",
      "\n",
      "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
      "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
      "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
      "\n",
      "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
      "\n",
      "# Ensure the output folder exists\n",
      "os.makedirs(\"cutoffs\", exist_ok=True)\n",
      "\n",
      "# -------------------------------\n",
      "# 2. Compute cumulative loss curves\n",
      "# -------------------------------\n",
      "cutoff_results = {}\n",
      "base_train_size = len(x_base_train)  # size of the base training data\n",
      "\n",
      "for model_data in all_models:\n",
      "    model = model_data[\"model\"]\n",
      "    additional_data = model_data[\"additional_data\"]\n",
      "\n",
      "    # Compute cumulative average loss curve\n",
      "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
      "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
      "\n",
      "    # Indices (1-based) for plotting\n",
      "    indices = list(range(1, len(curve) + 1))\n",
      "\n",
      "    # Store results in dictionary for JSON saving\n",
      "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
      "        \"additional_data\": additional_data,\n",
      "        \"base_train_size\": base_train_size,\n",
      "        \"indices\": indices,\n",
      "        \"loss_curve\": curve.tolist()\n",
      "    }\n",
      "\n",
      "# -------------------------------\n",
      "# 3. Save JSON results\n",
      "# -------------------------------\n",
      "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
      "with open(json_path, \"w\") as f:\n",
      "    json.dump(cutoff_results, f, indent=2)\n",
      "\n",
      "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
      "\n",
      "# -------------------------------\n",
      "# 4. Plot cumulative loss curves\n",
      "# -------------------------------\n",
      "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "for model_name, data in cutoff_results.items():\n",
      "    indices = data[\"indices\"]\n",
      "    loss_curve = data[\"loss_curve\"]\n",
      "    additional_data = data[\"additional_data\"]\n",
      "    base_train_size = data[\"base_train_size\"]\n",
      "\n",
      "    # Plot the cumulative average loss curve\n",
      "    plt.plot(indices, loss_curve, label=model_name)\n",
      "\n",
      "    # Add vertical line indicating the dataset size that model was trained on\n",
      "    train_size = base_train_size + additional_data\n",
      "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
      "\n",
      "# Configure plot aesthetics\n",
      "plt.xscale(\"log\")\n",
      "plt.xlabel(\"Number of samples (log scale)\")\n",
      "plt.ylabel(\"Cumulative average loss\")\n",
      "plt.title(\"Cumulative Loss Curves for All Models\")\n",
      "plt.legend(title=\"Models\")\n",
      "plt.grid(True, alpha=0.3)\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(plot_path, dpi=300)\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stdout -----\n",
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cumulative loss curves JSON to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Plot cumulative loss curves\u001b[39;00m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device (took 8.01s)\n",
      "üîπ Running model_5_data_15\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_5_data_15/Volume Estimation Pipeline.ipynb (took 13.45s)\n",
      "üîπ Running model_6_data_16\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_6_data_16/Test Accuracy.ipynb (took 7.43s)\n",
      "üîπ Running model_6_data_16\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Running model_6_data_16\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: model_6_data_16/Volume Cutoff.ipynb - [NbConvertApp] Converting notebook model_6_data_16\\Volume Cutoff.ipynb to notebook\n",
      "L:\\Programming\\diffusion-env\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"L:\\Programming\\diffusion-env\\Scripts\\jupyter-nbconvert.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 1062, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 918, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "# -------------------------------\n",
      "# 1. Build the full dataset once\n",
      "# -------------------------------\n",
      "# Explicitly extract tensors from the loaded dataset\n",
      "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
      "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
      "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
      "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
      "x_test = loaded_dataset.get(\"x_test\", None)\n",
      "y_test = loaded_dataset.get(\"y_test\", None)\n",
      "\n",
      "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
      "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
      "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
      "\n",
      "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
      "\n",
      "# Ensure the output folder exists\n",
      "os.makedirs(\"cutoffs\", exist_ok=True)\n",
      "\n",
      "# -------------------------------\n",
      "# 2. Compute cumulative loss curves\n",
      "# -------------------------------\n",
      "cutoff_results = {}\n",
      "base_train_size = len(x_base_train)  # size of the base training data\n",
      "\n",
      "for model_data in all_models:\n",
      "    model = model_data[\"model\"]\n",
      "    additional_data = model_data[\"additional_data\"]\n",
      "\n",
      "    # Compute cumulative average loss curve\n",
      "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
      "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
      "\n",
      "    # Indices (1-based) for plotting\n",
      "    indices = list(range(1, len(curve) + 1))\n",
      "\n",
      "    # Store results in dictionary for JSON saving\n",
      "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
      "        \"additional_data\": additional_data,\n",
      "        \"base_train_size\": base_train_size,\n",
      "        \"indices\": indices,\n",
      "        \"loss_curve\": curve.tolist()\n",
      "    }\n",
      "\n",
      "# -------------------------------\n",
      "# 3. Save JSON results\n",
      "# -------------------------------\n",
      "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
      "with open(json_path, \"w\") as f:\n",
      "    json.dump(cutoff_results, f, indent=2)\n",
      "\n",
      "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
      "\n",
      "# -------------------------------\n",
      "# 4. Plot cumulative loss curves\n",
      "# -------------------------------\n",
      "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "for model_name, data in cutoff_results.items():\n",
      "    indices = data[\"indices\"]\n",
      "    loss_curve = data[\"loss_curve\"]\n",
      "    additional_data = data[\"additional_data\"]\n",
      "    base_train_size = data[\"base_train_size\"]\n",
      "\n",
      "    # Plot the cumulative average loss curve\n",
      "    plt.plot(indices, loss_curve, label=model_name)\n",
      "\n",
      "    # Add vertical line indicating the dataset size that model was trained on\n",
      "    train_size = base_train_size + additional_data\n",
      "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
      "\n",
      "# Configure plot aesthetics\n",
      "plt.xscale(\"log\")\n",
      "plt.xlabel(\"Number of samples (log scale)\")\n",
      "plt.ylabel(\"Cumulative average loss\")\n",
      "plt.title(\"Cumulative Loss Curves for All Models\")\n",
      "plt.legend(title=\"Models\")\n",
      "plt.grid(True, alpha=0.3)\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(plot_path, dpi=300)\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stdout -----\n",
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cumulative loss curves JSON to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Plot cumulative loss curves\u001b[39;00m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device (took 7.54s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_6_data_16/Volume Estimation Pipeline.ipynb (took 13.27s)\n",
      "üîπ Running model_7_data_17\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_7_data_17/Test Accuracy.ipynb (took 7.52s)\n",
      "üîπ Running model_7_data_17\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: model_7_data_17/Volume Cutoff.ipynb - [NbConvertApp] Converting notebook model_7_data_17\\Volume Cutoff.ipynb to notebook\n",
      "L:\\Programming\\diffusion-env\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"L:\\Programming\\diffusion-env\\Scripts\\jupyter-nbconvert.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 1062, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 918, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "# -------------------------------\n",
      "# 1. Build the full dataset once\n",
      "# -------------------------------\n",
      "# Explicitly extract tensors from the loaded dataset\n",
      "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
      "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
      "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
      "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
      "x_test = loaded_dataset.get(\"x_test\", None)\n",
      "y_test = loaded_dataset.get(\"y_test\", None)\n",
      "\n",
      "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
      "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
      "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
      "\n",
      "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
      "\n",
      "# Ensure the output folder exists\n",
      "os.makedirs(\"cutoffs\", exist_ok=True)\n",
      "\n",
      "# -------------------------------\n",
      "# 2. Compute cumulative loss curves\n",
      "# -------------------------------\n",
      "cutoff_results = {}\n",
      "base_train_size = len(x_base_train)  # size of the base training data\n",
      "\n",
      "for model_data in all_models:\n",
      "    model = model_data[\"model\"]\n",
      "    additional_data = model_data[\"additional_data\"]\n",
      "\n",
      "    # Compute cumulative average loss curve\n",
      "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
      "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
      "\n",
      "    # Indices (1-based) for plotting\n",
      "    indices = list(range(1, len(curve) + 1))\n",
      "\n",
      "    # Store results in dictionary for JSON saving\n",
      "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
      "        \"additional_data\": additional_data,\n",
      "        \"base_train_size\": base_train_size,\n",
      "        \"indices\": indices,\n",
      "        \"loss_curve\": curve.tolist()\n",
      "    }\n",
      "\n",
      "# -------------------------------\n",
      "# 3. Save JSON results\n",
      "# -------------------------------\n",
      "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
      "with open(json_path, \"w\") as f:\n",
      "    json.dump(cutoff_results, f, indent=2)\n",
      "\n",
      "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
      "\n",
      "# -------------------------------\n",
      "# 4. Plot cumulative loss curves\n",
      "# -------------------------------\n",
      "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "for model_name, data in cutoff_results.items():\n",
      "    indices = data[\"indices\"]\n",
      "    loss_curve = data[\"loss_curve\"]\n",
      "    additional_data = data[\"additional_data\"]\n",
      "    base_train_size = data[\"base_train_size\"]\n",
      "\n",
      "    # Plot the cumulative average loss curve\n",
      "    plt.plot(indices, loss_curve, label=model_name)\n",
      "\n",
      "    # Add vertical line indicating the dataset size that model was trained on\n",
      "    train_size = base_train_size + additional_data\n",
      "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
      "\n",
      "# Configure plot aesthetics\n",
      "plt.xscale(\"log\")\n",
      "plt.xlabel(\"Number of samples (log scale)\")\n",
      "plt.ylabel(\"Cumulative average loss\")\n",
      "plt.title(\"Cumulative Loss Curves for All Models\")\n",
      "plt.legend(title=\"Models\")\n",
      "plt.grid(True, alpha=0.3)\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(plot_path, dpi=300)\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stdout -----\n",
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cumulative loss curves JSON to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Plot cumulative loss curves\u001b[39;00m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device (took 7.51s)\n",
      "üîπ Running model_7_data_17\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_7_data_17/Volume Estimation Pipeline.ipynb (took 13.71s)\n",
      "üîπ Running model_8_data_18\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_8_data_18/Test Accuracy.ipynb (took 7.52s)\n",
      "üîπ Running model_8_data_18\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: model_8_data_18/Volume Cutoff.ipynb - [NbConvertApp] Converting notebook model_8_data_18\\Volume Cutoff.ipynb to notebook\n",
      "L:\\Programming\\diffusion-env\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"L:\\Programming\\diffusion-env\\Scripts\\jupyter-nbconvert.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 1062, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 918, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "# -------------------------------\n",
      "# 1. Build the full dataset once\n",
      "# -------------------------------\n",
      "# Explicitly extract tensors from the loaded dataset\n",
      "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
      "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
      "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
      "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
      "x_test = loaded_dataset.get(\"x_test\", None)\n",
      "y_test = loaded_dataset.get(\"y_test\", None)\n",
      "\n",
      "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
      "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
      "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
      "\n",
      "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
      "\n",
      "# Ensure the output folder exists\n",
      "os.makedirs(\"cutoffs\", exist_ok=True)\n",
      "\n",
      "# -------------------------------\n",
      "# 2. Compute cumulative loss curves\n",
      "# -------------------------------\n",
      "cutoff_results = {}\n",
      "base_train_size = len(x_base_train)  # size of the base training data\n",
      "\n",
      "for model_data in all_models:\n",
      "    model = model_data[\"model\"]\n",
      "    additional_data = model_data[\"additional_data\"]\n",
      "\n",
      "    # Compute cumulative average loss curve\n",
      "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
      "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
      "\n",
      "    # Indices (1-based) for plotting\n",
      "    indices = list(range(1, len(curve) + 1))\n",
      "\n",
      "    # Store results in dictionary for JSON saving\n",
      "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
      "        \"additional_data\": additional_data,\n",
      "        \"base_train_size\": base_train_size,\n",
      "        \"indices\": indices,\n",
      "        \"loss_curve\": curve.tolist()\n",
      "    }\n",
      "\n",
      "# -------------------------------\n",
      "# 3. Save JSON results\n",
      "# -------------------------------\n",
      "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
      "with open(json_path, \"w\") as f:\n",
      "    json.dump(cutoff_results, f, indent=2)\n",
      "\n",
      "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
      "\n",
      "# -------------------------------\n",
      "# 4. Plot cumulative loss curves\n",
      "# -------------------------------\n",
      "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "for model_name, data in cutoff_results.items():\n",
      "    indices = data[\"indices\"]\n",
      "    loss_curve = data[\"loss_curve\"]\n",
      "    additional_data = data[\"additional_data\"]\n",
      "    base_train_size = data[\"base_train_size\"]\n",
      "\n",
      "    # Plot the cumulative average loss curve\n",
      "    plt.plot(indices, loss_curve, label=model_name)\n",
      "\n",
      "    # Add vertical line indicating the dataset size that model was trained on\n",
      "    train_size = base_train_size + additional_data\n",
      "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
      "\n",
      "# Configure plot aesthetics\n",
      "plt.xscale(\"log\")\n",
      "plt.xlabel(\"Number of samples (log scale)\")\n",
      "plt.ylabel(\"Cumulative average loss\")\n",
      "plt.title(\"Cumulative Loss Curves for All Models\")\n",
      "plt.legend(title=\"Models\")\n",
      "plt.grid(True, alpha=0.3)\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(plot_path, dpi=300)\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stdout -----\n",
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cumulative loss curves JSON to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Plot cumulative loss curves\u001b[39;00m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device (took 7.37s)\n",
      "üîπ Running model_8_data_18\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_8_data_18/Volume Estimation Pipeline.ipynb (took 13.78s)\n",
      "üîπ Running model_9_data_19\\Test Accuracy.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_9_data_19/Test Accuracy.ipynb (took 7.54s)\n",
      "üîπ Running model_9_data_19\\Volume Cutoff.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: model_9_data_19/Volume Cutoff.ipynb - [NbConvertApp] Converting notebook model_9_data_19\\Volume Cutoff.ipynb to notebook\n",
      "L:\\Programming\\diffusion-env\\Lib\\site-packages\\zmq\\_future.py:724: RuntimeWarning: Proactor event loop does not implement add_reader family of methods required for zmq. Registering an additional selector thread for add_reader support via tornado. Use `asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())` to avoid this warning.\n",
      "  self._get_loop()\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"L:\\Programming\\diffusion-env\\Scripts\\jupyter-nbconvert.EXE\\__main__.py\", line 7, in <module>\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbconvert\\preprocessors\\execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\jupyter_core\\utils\\__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 1062, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"L:\\Programming\\diffusion-env\\Lib\\site-packages\\nbclient\\client.py\", line 918, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "# -------------------------------\n",
      "# 1. Build the full dataset once\n",
      "# -------------------------------\n",
      "# Explicitly extract tensors from the loaded dataset\n",
      "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
      "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
      "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
      "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
      "x_test = loaded_dataset.get(\"x_test\", None)\n",
      "y_test = loaded_dataset.get(\"y_test\", None)\n",
      "\n",
      "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
      "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
      "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
      "\n",
      "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
      "\n",
      "# Ensure the output folder exists\n",
      "os.makedirs(\"cutoffs\", exist_ok=True)\n",
      "\n",
      "# -------------------------------\n",
      "# 2. Compute cumulative loss curves\n",
      "# -------------------------------\n",
      "cutoff_results = {}\n",
      "base_train_size = len(x_base_train)  # size of the base training data\n",
      "\n",
      "for model_data in all_models:\n",
      "    model = model_data[\"model\"]\n",
      "    additional_data = model_data[\"additional_data\"]\n",
      "\n",
      "    # Compute cumulative average loss curve\n",
      "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
      "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
      "\n",
      "    # Indices (1-based) for plotting\n",
      "    indices = list(range(1, len(curve) + 1))\n",
      "\n",
      "    # Store results in dictionary for JSON saving\n",
      "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
      "        \"additional_data\": additional_data,\n",
      "        \"base_train_size\": base_train_size,\n",
      "        \"indices\": indices,\n",
      "        \"loss_curve\": curve.tolist()\n",
      "    }\n",
      "\n",
      "# -------------------------------\n",
      "# 3. Save JSON results\n",
      "# -------------------------------\n",
      "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
      "with open(json_path, \"w\") as f:\n",
      "    json.dump(cutoff_results, f, indent=2)\n",
      "\n",
      "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
      "\n",
      "# -------------------------------\n",
      "# 4. Plot cumulative loss curves\n",
      "# -------------------------------\n",
      "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "for model_name, data in cutoff_results.items():\n",
      "    indices = data[\"indices\"]\n",
      "    loss_curve = data[\"loss_curve\"]\n",
      "    additional_data = data[\"additional_data\"]\n",
      "    base_train_size = data[\"base_train_size\"]\n",
      "\n",
      "    # Plot the cumulative average loss curve\n",
      "    plt.plot(indices, loss_curve, label=model_name)\n",
      "\n",
      "    # Add vertical line indicating the dataset size that model was trained on\n",
      "    train_size = base_train_size + additional_data\n",
      "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
      "\n",
      "# Configure plot aesthetics\n",
      "plt.xscale(\"log\")\n",
      "plt.xlabel(\"Number of samples (log scale)\")\n",
      "plt.ylabel(\"Cumulative average loss\")\n",
      "plt.title(\"Cumulative Loss Curves for All Models\")\n",
      "plt.legend(title=\"Models\")\n",
      "plt.grid(True, alpha=0.3)\n",
      "\n",
      "# Save the figure\n",
      "plt.savefig(plot_path, dpi=300)\n",
      "plt.show()\n",
      "plt.close()\n",
      "\n",
      "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n",
      "\n",
      "------------------\n",
      "\n",
      "----- stdout -----\n",
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n",
      "------------------\n",
      "\n",
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;32m---> 51\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcutoff_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved cumulative loss curves JSON to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 4. Plot cumulative loss curves\u001b[39;00m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n",
      "\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n",
      "\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n",
      "\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device (took 7.49s)\n",
      "üîπ Running model_9_data_19\\Volume Estimation Pipeline.ipynb...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: model_9_data_19/Volume Estimation Pipeline.ipynb (took 13.40s)\n",
      "‚úÖ All done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "MAX_WORKERS = 1\n",
    "EXCLUDE_PREFIXES = [\n",
    "    '__pycache__', \n",
    "    '.ipynb_checkpoints', \n",
    "    'base', \n",
    "    'completed runs', \n",
    "    'to do', \n",
    "    'analysis'\n",
    "]\n",
    "\n",
    "# Lock for thread-safe printing\n",
    "print_lock = Lock()\n",
    "\n",
    "# ---------- Core Helpers ----------\n",
    "def run_notebook(folder: str, notebook_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a Jupyter notebook in place using nbconvert.\n",
    "    Returns a status string with execution time.\n",
    "    All printing inside this function is thread-safe.\n",
    "    \"\"\"\n",
    "    notebook_path = os.path.join(folder, notebook_name)\n",
    "    if not os.path.exists(notebook_path):\n",
    "        with print_lock:\n",
    "            print(f\"SKIP: {notebook_name} not found in {folder}\", flush=True)\n",
    "        return f\"SKIP: {notebook_name} not found in {folder}\"\n",
    "\n",
    "    with print_lock:\n",
    "        print(f\"üîπ Running {notebook_path}...\", flush=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                'jupyter', 'nbconvert', '--to', 'notebook',\n",
    "                '--execute', notebook_path, '--inplace',\n",
    "                '--ExecutePreprocessor.timeout=-1'\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        return f\"‚úÖ SUCCESS: {folder}/{notebook_name} (took {elapsed:.2f}s)\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        stderr_clean = e.stderr.strip()\n",
    "        return f\"‚ùå ERROR: {folder}/{notebook_name} - {stderr_clean} (took {elapsed:.2f}s)\"\n",
    "\n",
    "def find_folders(base_dir: str = '.') -> list[str]:\n",
    "    \"\"\"Return a list of folders to process, excluding certain prefixes.\"\"\"\n",
    "    return [\n",
    "        d for d in os.listdir(base_dir)\n",
    "        if os.path.isdir(d) and not any(d.startswith(prefix) for prefix in EXCLUDE_PREFIXES)\n",
    "    ]\n",
    "\n",
    "def run_in_parallel(folders: list[str], notebook_names: list[str]):\n",
    "    \"\"\"Run one or more notebooks across all folders in parallel, with thread-safe printing.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [\n",
    "            executor.submit(run_notebook, folder, nb)\n",
    "            for folder in folders\n",
    "            for nb in notebook_names\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            msg = future.result().strip()\n",
    "            with print_lock:\n",
    "                print(msg)\n",
    "    with print_lock:\n",
    "        print(\"‚úÖ All done!\")\n",
    "\n",
    "# ---------- Specialized Variants ----------\n",
    "def run_random_perturbs_parallel():\n",
    "    \"\"\"Run Random Perturbs.ipynb across all model_* folders.\"\"\"\n",
    "    folders = find_folders()\n",
    "    with print_lock:\n",
    "        print(f\"Found {len(folders)} folders for Random Perturbs.\")\n",
    "    run_in_parallel(folders, [\"Random Perturbs.ipynb\"])\n",
    "\n",
    "def run_volume_parallel():\n",
    "    \"\"\"\n",
    "    Run all notebooks containing specific keywords (Volume Estimation, Volume Cutoff, Test Accuracy)\n",
    "    across all folders. Easy to comment out unwanted notebook types by editing valid_names list.\n",
    "    \"\"\"\n",
    "    folders = find_folders()\n",
    "    with print_lock:\n",
    "        print(f\"Found {len(folders)} folders for Volume/Test Accuracy notebooks.\")\n",
    "\n",
    "    # ‚úÖ Editable list of allowed notebook name substrings\n",
    "    valid_names = [\n",
    "        \"Volume Estimation\",\n",
    "        #\"Volume Cutoff\",\n",
    "        #\"Test Accuracy\"\n",
    "    ]\n",
    "\n",
    "    notebook_names = []\n",
    "    if folders:\n",
    "        reference_folder = folders[0]\n",
    "        for nb in os.listdir(reference_folder):\n",
    "            if nb.endswith(\".ipynb\") and any(name in nb for name in valid_names):\n",
    "                notebook_names.append(nb)\n",
    "\n",
    "    if not notebook_names:\n",
    "        with print_lock:\n",
    "            print(\"‚ö†Ô∏è No matching notebooks found in reference folder.\")\n",
    "        return\n",
    "\n",
    "    with print_lock:\n",
    "        print(f\"Will run these notebooks: {notebook_names}\")\n",
    "    run_in_parallel(folders, notebook_names)\n",
    "\n",
    "\n",
    "def run_nb_parallel():\n",
    "    \"\"\"Generic function for ad-hoc notebook runs (prompt user).\"\"\"\n",
    "    notebook_name = input(\"Enter notebook name (without .ipynb): \").strip() + \".ipynb\"\n",
    "    folders = find_folders()\n",
    "    with print_lock:\n",
    "        print(f\"Found {len(folders)} folders to process.\")\n",
    "    run_in_parallel(folders, [notebook_name])\n",
    "\n",
    "run_volume_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca324062-5ff0-478e-acee-9074606796f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
