{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1bf684-d8fa-4de8-b136-a894b6f0f176",
   "metadata": {},
   "source": [
    "# Comparisons In A Given Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9c9cf7-eb85-456f-b84e-e94d457fe560",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "cutoffs.json not found at model_0_data_10\\cutoffs\\cutoffs.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Data for the loss landscape levels\u001b[39;00m\n\u001b[0;32m     30\u001b[0m experiment_folders, data_modifications \u001b[38;5;241m=\u001b[39m grab_folder_names()\n\u001b[1;32m---> 32\u001b[0m base_train_size \u001b[38;5;241m=\u001b[39m \u001b[43mload_base_train_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_folders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m model_data_sizes \u001b[38;5;241m=\u001b[39m list_additional_data(experiment_folders[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     34\u001b[0m num_params \u001b[38;5;241m=\u001b[39m load_param_num(experiment_folders[\u001b[38;5;241m0\u001b[39m], data_modifications[\u001b[38;5;241m0\u001b[39m], loss_value \u001b[38;5;241m=\u001b[39m loss_value)\n",
      "File \u001b[1;32mL:\\Programming\\ARC\\minima_volume_project\\minima_volume\\analysis_funcs.py:186\u001b[0m, in \u001b[0;36mload_base_train_size\u001b[1;34m(experiment_folder)\u001b[0m\n\u001b[0;32m    184\u001b[0m cutoff_path \u001b[38;5;241m=\u001b[39m Path(experiment_folder) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cutoff_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoffs.json not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcutoff_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cutoff_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    189\u001b[0m     cutoff_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: cutoffs.json not found at model_0_data_10\\cutoffs\\cutoffs.json"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from minima_volume.analysis_funcs import (  grab_folder_names,\n",
    "                                            load_base_train_size,\n",
    "                                            list_additional_data,\n",
    "                                            load_param_num,\n",
    "                                            multiple_minima_fixed_landscape,\n",
    "                                            fixed_landscape_minima_labels,\n",
    "                                            plot_fixed_landscape_minima_pair,\n",
    "                                            plot_perturb_probs,\n",
    "                                            model_volume_across_targets,\n",
    "                                            append_cutoff_points,\n",
    "                                            varying_landscape_minima_labels,\n",
    "                                            plot_minima_volume_vs_data_level,                    \n",
    ")\n",
    "\n",
    "problem_name = \"CIFAR10 CNN\"\n",
    "\n",
    "loss_value = 0.1  # Loss value to analyze\n",
    "data_level_log = False #Is the data log distributed?\n",
    "base_output_dir = \"analysis\"  # Root folder to save all plots\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Data for the loss landscape levels\n",
    "experiment_folders, data_modifications = grab_folder_names()\n",
    "\n",
    "base_train_size = load_base_train_size(experiment_folders[0])\n",
    "model_data_sizes = list_additional_data(experiment_folders[0])\n",
    "num_params = load_param_num(experiment_folders[0], data_modifications[0], loss_value = loss_value)\n",
    "\n",
    "base_shift = 0\n",
    "if data_modifications[0].startswith(\"data_\"):\n",
    "    base_shift = base_train_size  # Shift to reflect true dataset size\n",
    "\n",
    "print(\"Experiment folders: \", experiment_folders)\n",
    "print(\"Different data levels where loss landscapes were computed:\", data_modifications)\n",
    "print(\"The base train size is: \", base_train_size)\n",
    "print(\"Data levels where models were trained: \", model_data_sizes)\n",
    "print(\"The number of model parameters is \", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7f87f-9623-4855-84db-09f38d813958",
   "metadata": {},
   "source": [
    "## Plotting Different Model Volumes In A Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf619c2-064d-4483-b902-e23aaf0e8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for fixed colors for background plots\n",
    "import matplotlib as mpl\n",
    "\n",
    "#cmap = mpl.colormaps.get_cmap(\"winter\")  # blue → green\n",
    "#background_colors = [cmap(i) for i in np.linspace(0, 1, 10)]\n",
    "\n",
    "#natural_label = \"Minima (On Base Dataset)\"\n",
    "if data_modifications[0].startswith(\"data_\"):\n",
    "    other_label = \"Minima (Larger Datasets)\"\n",
    "else:\n",
    "    other_label = \"Minima (Poisoned Datasets)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f4371-dbed-4dfe-83ae-96ed3816a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_modification in data_modifications:\n",
    "    print(f\"\\nProcessing data modification folder: {data_modification}\")\n",
    "\n",
    "    # Create output folder for this data_modification\n",
    "    save_dir = os.path.join(base_output_dir, data_modification)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Get axis labels and titles\n",
    "    labels = fixed_landscape_minima_labels(data_modification, base_train_size)\n",
    "\n",
    "    # Extract minima landscapes\n",
    "    all_mdl, all_log_rn, all_test_loss = multiple_minima_fixed_landscape(\n",
    "        experiment_folders, data_modification, loss_value #mdl is modification data level\n",
    "    )\n",
    "    number_of_minima = len(all_mdl[0]) # Not used currently\n",
    "\n",
    "    all_mdl = np.array(all_mdl)\n",
    "    all_log_rn = np.array(all_log_rn)\n",
    "    all_test_loss = np.array(all_test_loss)\n",
    "    \n",
    "    # Print types and shapes\n",
    "    #for name, var in [(\"all_mdl\", all_mdl), (\"all_log_rn\", all_log_rn), (\"all_test_loss\", all_test_loss)]:\n",
    "    #    print(f\"{name}: type={type(var)}, shape={var.shape}\")\n",
    "\n",
    "    # Save the data using np.savez_compressed\n",
    "    save_dict = {\n",
    "        'all_mdl': all_mdl,\n",
    "        'all_log_rn': all_log_rn,\n",
    "        'all_test_loss': all_test_loss,\n",
    "        'data_modification': np.array([data_modification], dtype=object),\n",
    "        'base_train_size': np.array([base_train_size]),\n",
    "        'base_shift': np.array([base_shift]),\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    output_path = save_dir\n",
    "    filename = 'minima_data.npz'\n",
    "    np.savez_compressed(os.path.join(output_path, filename), **save_dict)\n",
    "    \n",
    "    print(f\"Saved data to {os.path.join(output_path, filename)}\")\n",
    "    \n",
    "    # Central tendency runs: mean and median\n",
    "    for ct, suffix in [(\"mean\", \"_avg\"), (\"median\", \"_median\")]:\n",
    "        for ranking in [False, True]:\n",
    "            out_dir = save_dir if not ranking else os.path.join(save_dir, \"ranks\") #handles ranking\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            # --- Data-level plots: loop through shaded + errorbar ---\n",
    "            #for avg_style in [\"shaded\", \"errorbar\"]:\n",
    "            #    style_suffix = \"_shaded\" if avg_style == \"shaded\" else \"_errbar\"\n",
    "            style_suffix = \"_errbar\"\n",
    "            avg_style = \"errorbar\"\n",
    "\n",
    "            # --- Data level vs Log Volume ---\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_mdl, all_log_rn,\n",
    "                xlabel=labels[\"xlabel\"], ylabel=\"Log Volume\",\n",
    "                title=f\"{problem_name} Minima Volumes\\n In {base_train_size + int(data_modification.split(\"_\")[1]):,} Example Loss Landscape\",\n",
    "                #labels[\"title_volume\"],\n",
    "                log_scale=data_level_log,\n",
    "                ranking=ranking,\n",
    "                alpha=0.7,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"data_level_vs_log_volume\" + suffix + style_suffix,\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=avg_style,\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=(avg_style == \"errorbar\"),\n",
    "                xlabel_size=14, ylabel_size=14, title_size=17,\n",
    "                base_shift=base_shift,\n",
    "                background_colors=None,#background_colors,     \n",
    "                natural_minima_loc = 'first',\n",
    "                natural_label=f\"Minima (Trained On {base_train_size + int(data_modification.split(\"_\")[1]):,} Examples)\",\n",
    "                other_label=other_label,               \n",
    "                natural_marker=\"^\",            \n",
    "                other_marker=\"o\",               \n",
    "            )\n",
    "\n",
    "            # --- Data level vs Test Loss ---\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_mdl, all_test_loss,\n",
    "                xlabel=labels[\"xlabel\"], ylabel=\"Test Loss\",\n",
    "                title=f\"{problem_name} Test Loss vs Dataset Size\",#labels[\"title_volume\"],\n",
    "                log_scale=False,\n",
    "                ranking=ranking,\n",
    "                alpha=0.5,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"data_level_vs_test_loss\" + suffix + style_suffix,\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=avg_style,\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=(avg_style == \"errorbar\"),\n",
    "                xlabel_size=14, ylabel_size=14, title_size=17,\n",
    "                base_shift=base_shift,\n",
    "                background_colors=None,#background_colors,     \n",
    "                natural_minima_loc = 'first',\n",
    "                natural_label=f\"Minima (Trained On {base_train_size + int(data_modification.split(\"_\")[1]):,} Examples)\", \n",
    "                other_label=other_label,               \n",
    "                natural_marker=\"^\",            \n",
    "                other_marker=\"o\",               \n",
    "            )\n",
    "\n",
    "            # --- Log Volume vs Test Loss ---\n",
    "            # Only error bar version (no shaded version)\n",
    "            plot_fixed_landscape_minima_pair(\n",
    "                all_log_rn, all_test_loss,\n",
    "                xlabel=\"Log Volume\", ylabel=\"Test Loss\",\n",
    "                title=f\"{problem_name} Minima Volumes\\n In {base_train_size + int(data_modification.split(\"_\")[1]):,} Example Loss Landscape\",\n",
    "                #labels[\"title_volume\"],\n",
    "                log_scale=False,\n",
    "                ranking=ranking,\n",
    "                alpha=0.5,\n",
    "                output_dir=out_dir,\n",
    "                filename=\"log_volume_vs_test_loss\" + suffix + \"_errbar\",\n",
    "                show_plot=not ranking,\n",
    "                plot_average=True,\n",
    "                average_style=\"errorbar\",\n",
    "                central_tendency=ct,\n",
    "                plot_x_error=True,  # allow x-error bars\n",
    "                xlabel_size=14, ylabel_size=14, title_size=17,\n",
    "                background_colors=None,#background_colors,              \n",
    "                #natural_minima_loc = 'last',\n",
    "                #natural_label=f\"Minima (Trained On {base_train_size + int(data_modification.split(\"_\")[1]):,} Examples)\", \n",
    "                #other_label=other_label,               \n",
    "                #natural_marker=\"^\",            \n",
    "                other_marker=\"o\",               \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63796f34-a5f0-4c4e-9088-6169971bd407",
   "metadata": {},
   "source": [
    "# Aggregrate Test Accuracy\n",
    "\n",
    "Test accuracy vs dataset size across all plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2503cd-f923-4797-9b85-75962b48b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_folders = [folder for folder in experiment_folders\n",
    "                   if not os.path.exists(os.path.join(folder, \"test_accuracies.npz\"))]\n",
    "\n",
    "if missing_folders:\n",
    "    print(f\"⚠️ Skipping aggregation — missing test_accuracies.npz in: {missing_folders}\")\n",
    "else:\n",
    "    # ---------- Step 2: Load all data ----------\n",
    "    all_dataset_sizes = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    for folder in experiment_folders:\n",
    "        file_path = os.path.join(folder, \"test_accuracies.npz\")\n",
    "        data = np.load(file_path)\n",
    "        all_dataset_sizes.extend(data[\"dataset_sizes\"])\n",
    "        all_accuracies.extend(data[\"final_test_accuracies\"])\n",
    "\n",
    "    all_dataset_sizes = np.array(all_dataset_sizes)\n",
    "    all_accuracies = np.array(all_accuracies)\n",
    "\n",
    "    # ---------- Step 3: Group by dataset size ----------\n",
    "    grouped_data = defaultdict(list)\n",
    "    for size, acc in zip(all_dataset_sizes, all_accuracies):\n",
    "        grouped_data[size].append(acc)\n",
    "\n",
    "    sorted_sizes = np.array(sorted(grouped_data.keys()))\n",
    "    mean_accs = np.array([np.mean(grouped_data[size]) for size in sorted_sizes])\n",
    "    std_accs = np.array([np.std(grouped_data[size]) for size in sorted_sizes])\n",
    "\n",
    "    # ---------- Step 4: Plot with error bars ----------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(\n",
    "        sorted_sizes, mean_accs, yerr=std_accs,\n",
    "        fmt='o-', capsize=4, linewidth=2, markersize=8,\n",
    "        color='tab:blue', ecolor='black', elinewidth=1, alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Training Dataset Size\", fontsize=12)\n",
    "    plt.ylabel(\"Final Test Accuracy\", fontsize=12)\n",
    "    plt.title(\"Test Accuracy vs. Dataset Size (Mean ± Std)\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    #plt.ylim((0.5, 1.0))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    plot_path = os.path.join(base_output_dir, \"test_acc_plot.png\")\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✅ Plot saved to {plot_path}\")\n",
    "\n",
    "    # ---------- Step 5: Save combined data ----------\n",
    "    save_path = os.path.join(base_output_dir, \"test_acc.npz\")\n",
    "    np.savez(\n",
    "        save_path,\n",
    "        dataset_sizes=sorted_sizes,\n",
    "        mean_accuracies=mean_accs,\n",
    "        std_accuracies=std_accs,\n",
    "        all_dataset_sizes=all_dataset_sizes,\n",
    "        all_accuracies=all_accuracies\n",
    "    )\n",
    "    print(f\"✅ Aggregated data saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f267ad-e4e1-4750-9944-25f8a9087a07",
   "metadata": {},
   "source": [
    "# Comparisons Across Loss Landscapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba89c53-f00e-4cdb-86c5-aaaf87e2cf16",
   "metadata": {},
   "source": [
    "Here, we track the performance of all models across all loss landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5ebe9-679a-43e0-9056-58de5b3779b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the poisoning experiments, this plot has no meaning\n",
    "\n",
    "labels = varying_landscape_minima_labels(\n",
    "    data_modification_folder=data_modifications[0], # Random choice to grab datatype from\n",
    "    base_train_size=base_train_size,\n",
    ")\n",
    "\n",
    "results_dict = model_volume_across_targets(\n",
    "    target_model_data_levels=model_data_sizes,\n",
    "    loss_value=loss_value,\n",
    "    experiment_folders=experiment_folders\n",
    ")\n",
    "results_with_cutoff = append_cutoff_points(results_dict, threshold=loss_value, base_dir=\".\")\n",
    "\n",
    "found_minima_vol, found_minima_dataset = plot_minima_volume_vs_data_level(\n",
    "    results_dict = results_with_cutoff,\n",
    "    data_type=labels[\"data_type\"],        \n",
    "    base_train_size=base_train_size, \n",
    "    xlabel=labels[\"xlabel\"],\n",
    "    ylabel=\"Log Volume\",\n",
    "    suptitle=f\"{problem_name}\",\n",
    "    title=f\"Minima Volumes Across Datasets\",\n",
    "    log_scale = data_level_log,\n",
    "    alpha=0.1,\n",
    "    plot_average=True,\n",
    "    output_dir=base_output_dir, filename=\"log_volumes_vs_data_levels\",\n",
    "    xlabel_size=15, ylabel_size=15, title_size=15, suptitle_size=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe705cd-ad01-457f-b8fc-864d6ba97329",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, = plot_minima_volume_vs_data_level(\n",
    "    results_dict = results_with_cutoff,\n",
    "    data_type=labels[\"data_type\"],          \n",
    "    base_train_size=base_train_size, \n",
    "    xlabel=labels[\"xlabel\"],\n",
    "    ylabel=\"Log Volume\",\n",
    "    #suptitle=f\"{problem_name}\",\n",
    "    title=f\"{problem_name}\\nMinima Volumes Across Datasets\",\n",
    "    log_scale = True,\n",
    "    alpha=0.1,\n",
    "    plot_average=True,\n",
    "    output_dir=base_output_dir, filename=\"log_volumes_vs_data_levels_log\",\n",
    "    xlabel_size=15, ylabel_size=15, title_size=15, suptitle_size=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fa122d-7cdd-405f-b452-01c42200c41c",
   "metadata": {},
   "source": [
    "## Customized Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fad84-c1c6-424e-ae3d-635ac069f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We experiment a little with making customized graphs.\n",
    "\n",
    "_, _, = plot_minima_volume_vs_data_level(\n",
    "    results_dict = results_with_cutoff,\n",
    "    data_type=labels[\"data_type\"],          \n",
    "    base_train_size=base_train_size, \n",
    "    xlabel=labels[\"xlabel\"],\n",
    "    ylabel=\"Log Volume\",\n",
    "    #suptitle=f\"{problem_name}\",\n",
    "    title=f\"{problem_name}\\nMinima Volumes Across Datasets\",\n",
    "    log_scale = True,\n",
    "    alpha=0.1,\n",
    "    plot_average=True,\n",
    "    output_dir=base_output_dir, filename=\"log_volumes_vs_data_levels_log_custom\",\n",
    "    xlabel_size=15, ylabel_size=15, title_size=15, suptitle_size=18,\n",
    "    ylim = (250000, 1000000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cc389-e620-4444-b261-88b0adc41831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "x = np.array(found_minima_vol)\n",
    "y = np.array(found_minima_dataset)\n",
    "\n",
    "# Fit a linear relationship: y = n * log(x) + log(k)\n",
    "coefficients = np.polyfit(np.log(x), y, 1)\n",
    "\n",
    "# Extract the exponent n and constant log(k)\n",
    "exponent = coefficients[0]  # This is your exponent!\n",
    "log_k = coefficients[1]  # log of the constant\n",
    "\n",
    "print(f\"Fitting to y = a * log(x) + log(k) \")\n",
    "print(f\"Exponent a = {exponent:.4f}\")\n",
    "print(f\"Log k = {log_k:.4f}\")\n",
    "#print(f\"Power law relationship: z = {k:.4f} × x^{n:.4f}\")\n",
    "print(f\"y = {exponent:.4f} × log(x) + {log_k:.4f}\")\n",
    "\n",
    "print(\"The exponent divided by the number of parameters is \", exponent/num_params)\n",
    "\n",
    "# Create the fit line for plotting\n",
    "x_fit = np.linspace(min(x), max(x), 100)\n",
    "y_fit = exponent * np.log(x_fit) + log_k\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x, y, 'bo', label=\"Minima Volumes\")\n",
    "plt.plot(x_fit, y_fit, 'k-', label=f'Fit: y = {exponent:.3f}·log(x) + {log_k:.3f}')\n",
    "#plt.plot(xs, a*xs + b, '--', label=f\"Fit: y = {np.exp(b):.2f} * x^{a:.2f}\")\n",
    "#plt.loglog(xs, fit_line, '--', label=f\"Fit: y = {np.exp(b):.2f} * x^{a:.2f}\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Dataset Size\")\n",
    "plt.ylabel(\"Log Volume\")\n",
    "plt.title(f\"{problem_name} Scaling Law\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\")\n",
    "output_path = os.path.join(base_output_dir, \"scaling.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()  # Close the figure to free memory\n",
    "\n",
    "print(f\"Figure saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a20710-f069-4a7f-9072-84ba7230f43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
