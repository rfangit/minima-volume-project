{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4494701a-38ed-4662-8e6b-66b8e14fcadf",
   "metadata": {},
   "source": [
    "# Volume Cutoff\n",
    "\n",
    "Streamlined notebook for evaluating volume cutoffson saved models and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243f018-2c18-476a-82a6-10cdbce9e963",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b91824-82ff-4a5a-9a9c-434fa49310c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Local package imports\n",
    "from minima_volume.perturb_funcs import ( cumulative_average_loss_curve )\n",
    "\n",
    "from minima_volume.dataset_funcs import (\n",
    "    load_dataset,\n",
    "    load_model,\n",
    "    load_models_and_data,\n",
    "    prepare_datasets,\n",
    "    tensor_to_list,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6af0fc-d0a3-4793-8ca0-33c9461c6777",
   "metadata": {},
   "source": [
    "## Model + Dataset Specific Code\n",
    "\n",
    "This is for model and dataset specific code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389d62b5-59f9-43ff-9c59-8e57e3d39f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specifies the CIFAR-10 model module name\n",
    "import CIFAR10_CNN_model_data as model_module  # <- your new module for CIFAR-10\n",
    "\n",
    "# CIFAR-10 CNN initialization parameters\n",
    "conv_channels = [32, 64, 128]  # adjust as desired\n",
    "fc_dims = [512, 256]  # adjust as desired\n",
    "\n",
    "# Grab model - use CNN parameters instead of MLP hidden_dims\n",
    "model_template = model_module.get_model(\n",
    "    conv_channels=conv_channels,  # CNN-specific parameter\n",
    "    fc_dims=fc_dims,              # CNN-specific parameter\n",
    "    device=device, \n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# Grab loss and metrics\n",
    "loss_fn_per_sample = model_module.get_loss_fn_per_sample()\n",
    "#other_metrics = model_module.get_additional_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444f632-4ed7-4f3d-9f18-19a6cedabed8",
   "metadata": {},
   "source": [
    "## Loading Model and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47dff34d-5a53-49d0-a3df-d1306cecd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for models and dataset in: models_and_data\n",
      "Found 6 model files:\n",
      "  - model_additional_0.pt\n",
      "  - model_additional_1950.pt\n",
      "  - model_additional_19950.pt\n",
      "  - model_additional_450.pt\n",
      "  - model_additional_4950.pt\n",
      "  - model_additional_49950.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_0.pt\n",
      "Successfully loaded: model_additional_0.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_1950.pt\n",
      "Successfully loaded: model_additional_1950.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_19950.pt\n",
      "Successfully loaded: model_additional_19950.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_450.pt\n",
      "Successfully loaded: model_additional_450.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_4950.pt\n",
      "Successfully loaded: model_additional_4950.pt\n",
      "✅ Model loaded into provided instance from models_and_data\\model_additional_49950.pt\n",
      "Successfully loaded: model_additional_49950.pt\n",
      "\n",
      "Model data loaded from all models:\n",
      "Model 0 (model_additional_0.pt):\n",
      "  - Additional data: 0\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 1 (model_additional_1950.pt):\n",
      "  - Additional data: 1950\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 2 (model_additional_19950.pt):\n",
      "  - Additional data: 19950\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 3 (model_additional_450.pt):\n",
      "  - Additional data: 450\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 4 (model_additional_4950.pt):\n",
      "  - Additional data: 4950\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "Model 5 (model_additional_49950.pt):\n",
      "  - Additional data: 49950\n",
      "  - Dataset type: data\n",
      "  - Training accuracies: 200 entries\n",
      "  - Test accuracies: 200 entries\n",
      "\n",
      "Loading dataset...\n",
      "Using dataset file: dataset.pt\n",
      "✅ Dataset loaded from models_and_data\\dataset.pt\n",
      "Dataset type: data\n",
      "Dataset quantities: [0, 450, 1950, 4950, 19950, 49950]\n",
      "\n",
      "Tensor shapes:\n",
      "  x_base_train: torch.Size([50, 3, 32, 32])\n",
      "  y_base_train: torch.Size([50])\n",
      "  x_additional: torch.Size([49950, 3, 32, 32])\n",
      "  y_additional: torch.Size([49950])\n",
      "  x_test: torch.Size([10000, 3, 32, 32])\n",
      "  y_test: torch.Size([10000])\n",
      "Reconstructed 6 trained models\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Load Trained Models and Dataset\n",
    "# ====================================\n",
    "target_dir = \"models_and_data\"  # relative path\n",
    "loaded_models, loaded_model_data, loaded_dataset = load_models_and_data(\n",
    "    model_template=model_template,\n",
    "    target_dir=target_dir,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Dataset Info\n",
    "dataset_type = loaded_dataset['dataset_type']\n",
    "dataset_quantities = loaded_dataset['dataset_quantities']\n",
    "print(f\"Dataset type: {dataset_type}\")\n",
    "print(f\"Dataset quantities: {loaded_dataset['dataset_quantities']}\")\n",
    "\n",
    "print(\"\\nTensor shapes:\")\n",
    "for key in [\"x_base_train\", \"y_base_train\", \"x_additional\", \"y_additional\", \"x_test\", \"y_test\"]:\n",
    "    shape = getattr(loaded_dataset[key], \"shape\", None)\n",
    "    print(f\"  {key}: {shape if shape is not None else 'None'}\")\n",
    "\n",
    "# Reconstruct trained_model dicts safely.\n",
    "# If the loss or accuracy or additional metrics happen to be\n",
    "# tensors, they get safely converted to lists.\n",
    "all_models = [\n",
    "    {\n",
    "        \"model\": model,\n",
    "        **{\n",
    "            k: tensor_to_list(model_data[k], key_path=k)\n",
    "            for k in [\"train_loss\", \"train_accs\", \"test_loss\", \"test_accs\", \"additional_data\", \"dataset_type\"]\n",
    "        },\n",
    "    }\n",
    "    for model, model_data in zip(loaded_models, loaded_model_data)\n",
    "]\n",
    "print(f\"Reconstructed {len(all_models)} trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cfbf5-4a88-405f-91ee-d096e48c245c",
   "metadata": {},
   "source": [
    "## Loss Cutoff Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413116c6-56de-45b2-96cc-9c254e869cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating full dataset of size 50000 for all models...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn_per_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m additional_data \u001b[38;5;241m=\u001b[39m model_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute cumulative average loss curve\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m curve \u001b[38;5;241m=\u001b[39m cumulative_average_loss_curve(model, x_full, y_full, \u001b[43mloss_fn_per_sample\u001b[49m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Indices (1-based) for plotting\u001b[39;00m\n\u001b[0;32m     36\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(curve) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_fn_per_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Build the full dataset once\n",
    "# -------------------------------\n",
    "# Explicitly extract tensors from the loaded dataset\n",
    "x_base_train = loaded_dataset.get(\"x_base_train\", None)\n",
    "y_base_train = loaded_dataset.get(\"y_base_train\", None)\n",
    "x_additional = loaded_dataset.get(\"x_additional\", None)\n",
    "y_additional = loaded_dataset.get(\"y_additional\", None)\n",
    "x_test = loaded_dataset.get(\"x_test\", None)\n",
    "y_test = loaded_dataset.get(\"y_test\", None)\n",
    "\n",
    "max_additional = max(dataset_quantities)  # maximum additional data used across models\n",
    "x_full = torch.cat([x_base_train, x_additional[:max_additional]], dim=0)\n",
    "y_full = torch.cat([y_base_train, y_additional[:max_additional]], dim=0)\n",
    "\n",
    "print(f\"\\nEvaluating full dataset of size {len(x_full)} for all models...\")\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(\"cutoffs\", exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute cumulative loss curves\n",
    "# -------------------------------\n",
    "cutoff_results = {}\n",
    "base_train_size = len(x_base_train)  # size of the base training data\n",
    "\n",
    "for model_data in all_models:\n",
    "    model = model_data[\"model\"]\n",
    "    additional_data = model_data[\"additional_data\"]\n",
    "\n",
    "    # Compute cumulative average loss curve\n",
    "    # Note: loss_fn_per_sample must return per-sample losses (shape [batch_size])\n",
    "    curve = cumulative_average_loss_curve(model, x_full, y_full, loss_fn_per_sample)\n",
    "\n",
    "    # Indices (1-based) for plotting\n",
    "    indices = list(range(1, len(curve) + 1))\n",
    "\n",
    "    # Store results in dictionary for JSON saving\n",
    "    cutoff_results[f\"Model_{additional_data}\"] = {\n",
    "        \"additional_data\": additional_data,\n",
    "        \"base_train_size\": base_train_size,\n",
    "        \"indices\": indices,\n",
    "        \"loss_curve\": curve.tolist()\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Save JSON results\n",
    "# -------------------------------\n",
    "json_path = os.path.join(\"cutoffs\", \"cutoffs.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(cutoff_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved cumulative loss curves JSON to: {json_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Plot cumulative loss curves\n",
    "# -------------------------------\n",
    "plot_path = os.path.join(\"cutoffs\", \"loss_curves.png\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name, data in cutoff_results.items():\n",
    "    indices = data[\"indices\"]\n",
    "    loss_curve = data[\"loss_curve\"]\n",
    "    additional_data = data[\"additional_data\"]\n",
    "    base_train_size = data[\"base_train_size\"]\n",
    "\n",
    "    # Plot the cumulative average loss curve\n",
    "    plt.plot(indices, loss_curve, label=model_name)\n",
    "\n",
    "    # Add vertical line indicating the dataset size that model was trained on\n",
    "    train_size = base_train_size + additional_data\n",
    "    plt.axvline(train_size, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Configure plot aesthetics\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of samples (log scale)\")\n",
    "plt.ylabel(\"Cumulative average loss\")\n",
    "plt.title(\"Cumulative Loss Curves for All Models\")\n",
    "plt.legend(title=\"Models\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved cumulative loss curves plot to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae40734-4853-490b-8d4c-3ac2520f7aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diffusion-env)",
   "language": "python",
   "name": "diffusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
